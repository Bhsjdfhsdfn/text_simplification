Backup 1
        # remove below
        with tf.variable_scope("inputs"):
            # add <go> in the beginning of decoder
            self.sentence_simple_input = tf.stack(self.sentence_simple_input_placeholder, axis=1)
            simple_go = tf.reshape(tf.stack([self.data.vocab_simple.encode(constant.SYMBOL_GO)
                                             for _ in range(self.model_config.batch_size)], axis=0),
                                   [self.model_config.batch_size, -1])
            self.sentence_simple_input = tf.concat([simple_go, self.sentence_simple_input], axis=1)

            self.sentence_complex_input = tf.stack(self.sentence_complex_input_placeholder, axis=1)

            simple_input = tf.nn.embedding_lookup(self.emb_simple, self.sentence_simple_input)
            complex_input = tf.nn.embedding_lookup(self.emb_complex, self.sentence_complex_input)
            simple_input = common_attention.add_timing_signal_1d(simple_input)
            complex_input = common_attention.add_timing_signal_1d(complex_input)

            simple_input_bias = common_attention.attention_bias_lower_triangle(tf.shape(simple_input)[1])
            # simple_input_bias = common_attention.attention_bias_ignore_padding(
            #     tf.to_float(tf.equal(self.sentence_simple_input, self.data.vocab_simple.encode(constant.SYMBOL_PAD))))
            complex_input_bias = common_attention.attention_bias_ignore_padding(
                tf.to_float(tf.equal(self.sentence_complex_input, self.data.vocab_complex.encode(constant.SYMBOL_PAD))))

        with tf.variable_scope("transformer"):
            self.global_step = tf.get_variable('global_step',
                                               initializer=tf.constant(0, dtype=tf.int64), trainable=False)

            encoder = self.encoder(complex_input, complex_input_bias)

            if self.is_train:
                decoder = self.decoder(simple_input, encoder, simple_input_bias, complex_input_bias)
                output = self.output(decoder)
                with tf.variable_scope('optimization'):
                    self.increment_global_step = tf.assign_add(self.global_step, 1)
                    self.loss = sequence_loss(output, self.sentence_simple_input)
                    self.train_op = self.create_train_op()

            else:
                if self.model_config.beam_search_size <= 0:
                    decoder = self.decoder(simple_input, encoder, simple_input_bias, complex_input_bias)
                    output = self.output(decoder)
                    self.target = tf.argmax(output, axis=-1)
                    self.loss = sequence_loss(output, self.sentence_simple_input)
                else:
                    # Use Beam Search in evaluation stage
                    # Update [a, b, c] to [a, a, a, b, b, b, c, c, c] if beam_search_size == 3
                    complex_input = tf.concat(
                        [tf.tile(tf.expand_dims(complex_input[o, :, :], axis=0),
                                 [self.model_config.beam_search_size, 1, 1])
                         for o in range(self.model_config.batch_size)], axis=0)

                    complex_input_bias = tf.concat(
                        [tf.tile(tf.expand_dims(complex_input_bias[o, :, :, :], axis=0),
                                 [self.model_config.beam_search_size, 1, 1, 1])
                         for o in range(self.model_config.batch_size)], axis=0)

                    def beam_search_fn(ids):
                        embs = tf.nn.embedding_lookup(self.emb_simple, ids[:, 1:])
                        embs = tf.pad(embs, [[0, 0], [1, 0], [0, 0]])
                        embs_bias = common_attention.attention_bias_lower_triangle(tf.shape(embs)[1])
                        decoder = self.decoder(embs, complex_input, embs_bias, complex_input_bias)
                        output = self.output(decoder, w, b)
                        return output[:, -1, :]

                    beam_ids, beam_score = beam_search.beam_search(beam_search_fn,
                                                                   tf.zeros([self.model_config.batch_size], tf.int32),
                                                                   self.model_config.beam_search_size,
                                                                   self.model_config.max_simple_sentence,
                                                                   len(self.data.vocab_simple.i2w),
                                                                   0.6,
                                                                   self.data.vocab_simple.encode(constant.SYMBOL_END))
                    top_beam_ids = beam_ids[:, 0, 1:]
                    self.test_var = beam_ids
                    top_beam_ids = tf.pad(top_beam_ids,
                                          [[0, 0],
                                           [0, self.model_config.max_simple_sentence - tf.shape(top_beam_ids)[1]]])
                    self.target = [tf.squeeze(d, 1)
                                   for d in tf.split(top_beam_ids, self.model_config.max_simple_sentence, axis=1)]
                    # TODO(sanqiang) fix beam search loss
                    self.loss = tf.ones([1]) # -beam_score[:, 0] / tf.to_float(tf.shape(top_beam_ids)[1])
################################